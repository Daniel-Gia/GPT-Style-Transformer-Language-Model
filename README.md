# GPT-Style Transformer Language Model

## ğŸ¯ What This Project Is / Does

This project is a **complete implementation of a GPT-style transformer language model from scratch** using PyTorch. It demonstrates the fundamental architecture and training procedures used in modern large language models like GPT, ChatGPT, and similar systems.

### Key Features:
- **Custom Byte Pair Encoding (BPE) Tokenizer**: Implements subword tokenization from scratch
- **Transformer Architecture**: Multi-head self-attention, feed-forward networks, layer normalization
- **Training Pipeline**: Complete training loop with validation monitoring and checkpointing  
- **Text Generation**: Autoregressive text generation with sampling

## ğŸ“ Educational Purpose & Limitations

**This project is designed to demonstrate understanding of LLM/Attention Architecture fundamentals.**

âš ï¸ **Important Note**: The pretrained model included is just a **small example** and is **not great** for practical use. It's trained on a limited dataset with minimal parameters to showcase the architecture and training process.

**If you want better results:**
- Increase model parameters (embedding dimension, layers, heads)
- Train on larger, more diverse datasets
- Extend training time (more epochs/iterations)
- Tune hyperparameters (learning rate, batch size, etc.)

**For Training Larger Models:**
If you want to train bigger, more powerful models, I recommend using **Azure ML Studio** for cloud-based training with powerful GPUs. This is especially useful for:
- Models with millions/billions of parameters
- Large datasets that don't fit in local memory
- Faster training with high-end GPU clusters

ğŸ“º **Helpful Tutorial**: [Azure ML Studio Training Guide](https://www.youtube.com/watch?v=L16GXVOnqCM&list=WL) - Great tutorial for getting started with cloud-based model training.

This implementation prioritizes **educational clarity** over performance optimization.

## ğŸš€ How to Run

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (optional, but recommended for faster training)

### Installation
1. Clone or download this repository
2. Install required packages:
```bash
pip install -r requirements.txt
```
> **Note:** You may need to run this command with administrator privileges (e.g., use `sudo` on Linux/macOS or run your terminal as Administrator on Windows) if you encounter permission errors.

### Quick Start (Using Pretrained Model)
If you want to use the existing trained model for text generation:

```bash
python run.py
```
Enter your text prompt when asked, and the model will generate a continuation.

### Training From Scratch
If you want to train the model yourself:

1. **Initialize the tokenizer** (creates vocabulary and merge rules):
```bash
python init.py
```

2. **Train the model** (this will take time, especially on CPU):
```bash
python train.py
```

3. **Generate text** with your trained model:
```bash
python run.py
```

### Training Data
The model is trained on Shakespeare text (`Train_Data/input.txt`). You can replace this with your own text data to train on different content.

## ğŸ“ Project Structure

```
llm-example/
â”œâ”€â”€ ğŸ“„ README.md                   # This file
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python package dependencies
â”œâ”€â”€ ğŸ“„ Model_Params.py            # Configuration parameters and hyperparameters
â”œâ”€â”€ ğŸ“„ Model.py                   # Transformer architecture implementation
â”œâ”€â”€ ğŸ“„ tokenizer.py               # BPE tokenizer implementation
â”œâ”€â”€ ğŸ“„ init.py                    # Tokenizer initialization script
â”œâ”€â”€ ğŸ“„ train.py                   # Model training script
â”œâ”€â”€ ğŸ“„ run.py                     # Text generation/inference script
â”œâ”€â”€ ğŸ“ Train_Data/                # Training data directory
â”‚   â””â”€â”€ ğŸ“„ input.txt              # Shakespeare text for training
â”œâ”€â”€ ğŸ“ Saved_Weights/             # Model checkpoints directory
â”‚   â””â”€â”€ ğŸ“„ weights-*.pth          # Saved model weights
â””â”€â”€ ğŸ“ tokenizer_save/            # Tokenizer data directory
    â”œâ”€â”€ ğŸ“„ stoi.json              # String-to-integer mappings
    â”œâ”€â”€ ğŸ“„ itos.json              # Integer-to-string mappings  
    â”œâ”€â”€ ğŸ“„ merges.json            # BPE merge rules
    â””â”€â”€ ğŸ“„ vocab_size.json        # Vocabulary size information
```

### File Descriptions

#### Core Scripts
- **`Model_Params.py`**: Contains all hyperparameters and configuration settings
- **`Model.py`**: Implements the complete transformer architecture (attention, MLP, blocks)
- **`tokenizer.py`**: BPE tokenizer with encoding/decoding functionality
- **`init.py`**: Sets up and trains the tokenizer on your text data
- **`train.py`**: Main training loop with validation and checkpointing
- **`run.py`**: Interactive text generation using trained model

#### Data Directories
- **`Train_Data/`**: Contains training text data
- **`Saved_Weights/`**: Stores model checkpoints during training
- **`tokenizer_save/`**: Stores tokenizer vocabulary and merge rules

## ğŸ”§ Customization

### Model Architecture
Edit `Model_Params.py` to modify:
- `vocab_size`: Vocabulary size (affects tokenizer)
- `block_size`: Maximum sequence length
- `emb_len`: Embedding dimension (larger = more parameters)
- `num_layers`: Number of transformer layers
- `num_heads`: Number of attention heads
- `batch_size`: Training batch size
- `dropout`: Regularization strength

### Training Data
Replace `Train_Data/input.txt` with your own text data. The model will learn to generate text in the style of your training data.

## ğŸ§  Technical Details

### Architecture Components
1. **Token & Positional Embeddings**: Convert tokens to dense vectors with position information
2. **Multi-Head Self-Attention**: Allows model to attend to different parts of the sequence
3. **Feed-Forward Networks**: Process attended information through MLPs
4. **Layer Normalization**: Stabilizes training and improves performance
5. **Residual Connections**: Enable training of deeper networks

### Training Process
1. **Tokenization**: Text is converted to integer sequences using BPE
2. **Batch Creation**: Sequences are batched for parallel processing
3. **Forward Pass**: Model predicts next token probabilities
4. **Loss Calculation**: Cross-entropy loss between predictions and targets
5. **Backpropagation**: Gradients are computed and weights updated
6. **Validation**: Model performance is monitored on held-out data

## ğŸ“Š Model Performance

This implementation uses a small model configuration:
- **Parameters**: ~4M parameters
- **Training Data**: Shakespeare corpus (~1MB text)
- **Performance**: Basic text generation capabilities

For comparison, GPT-3 has 175B parameters and is trained on hundreds of gigabytes of text data.

## ğŸ¤ Contributing

This is an educational project. Feel free to:
- Experiment with different architectures
- Try different training data
- Implement additional features (beam search, top-k sampling, etc.)
- Optimize for better performance

## ğŸ“š References

This implementation is inspired by:
- "Attention Is All You Need" (Vaswani et al., 2017)

---

**Happy learning and experimenting with transformer architectures! ğŸš€**
